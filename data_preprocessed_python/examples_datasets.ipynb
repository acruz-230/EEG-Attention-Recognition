{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Introduction to the datasets Module\n\nWelcome to the guide on TorchEEG's ``datasets`` Module! This module provides you with various benchmark datasets for EEG-based emotion recognition, such as DEAP, DREAMER, SEED, MAHNOB, AMIGOS, and MPED. These datasets use a range of stimuli like music and videos to trigger emotional responses. Once the emotional experiences are recorded, participants label them using methods like valence-arousal dimensions or discrete emotion categories.\n\nThis guide will help you understand how to easily load and manipulate these datasets for your training needs, including signal segmentation, applying transformations, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An Example of Emotion Recognition Dataset\n\nTo begin, you'll need to download the DEAP dataset from DEAP download link. Once you have the dataset, use the following code to load it. This will read EEG signals and labels, apply offline transformations, and save them for easy access later on.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torcheeg.datasets import DEAPDataset\nfrom torcheeg import transforms\n\nfrom torcheeg.datasets.constants import \\\n    DEAP_CHANNEL_LOCATION_DICT\n\ndataset = DEAPDataset(\n    io_path=f'./deap',\n    root_path='./data_preprocessed_python',\n    offline_transform=transforms.Compose([\n        transforms.BandDifferentialEntropy(apply_to_baseline=True),\n        transforms.ToGrid(DEAP_CHANNEL_LOCATION_DICT, apply_to_baseline=True)\n    ]),\n    online_transform=transforms.Compose(\n        [transforms.BaselineRemoval(),\n         transforms.ToTensor()]),\n    label_transform=transforms.Compose([\n        transforms.Select('valence'),\n        transforms.Binary(5.0),\n    ]),\n    num_worker=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can get individual samples from the dataset like so:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will output a tuple, where the first element is the EEG signal and the second is its label.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to visualize the EEG signals, you can use the helper function as shown below:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torcheeg.utils import plot_3d_tensor\n\nimg = plot_3d_tensor(torch.tensor(dataset[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also refer to the document for more information on visualizing EEG signals\nvia the ``plot_3d_tensor``, ``plot_feature_topomap``, ``plot_raw_topomap``, ``plot_signal``, and ``plot_adj_connectivity`` function.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Notes\n\nIf you make changes to the dataset's configuration, remember to clear the cache or specify a new io_path. Otherwise, only online transformations (online_transform and label_transform) will take effect:\n\n```bash\n!rm -rf ./deap\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you remove all transforms and declare a dataset, you\u2019ll find that it\nreturns segmented EEG signals that haven\u2019t undergone any preprocessing.\nAdditionally, there is a dictionary representing all the\nmeta-information corresponding to the EEG signal samples.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torcheeg.datasets import DEAPDataset\n\ndataset = DEAPDataset(io_path=f'./deap', root_path='./data_preprocessed_python')\nprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage\n\nMost emotion analysis datasets can be found in TorchEEG. Moreover,\nTorchEEG provides support for ``MoABB``, allowing access to\nmotor-imagery-related datasets with the help of ``MoABB``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torcheeg.datasets.moabb as moabb_dataset\n\nfrom moabb.datasets import BNCI2014001\nfrom moabb.paradigms import LeftRightImagery\n\ndataset = BNCI2014001()\ndataset.subject_list = [1, 2, 3]\nparadigm = LeftRightImagery()\ndataset = moabb_dataset.MOABBDataset(\n    dataset=dataset,\n    paradigm=paradigm,\n    io_path='./moabb',\n    offline_transform=transforms.Compose([transforms.BandDifferentialEntropy()\n                                          ]),\n    online_transform=transforms.ToTensor(),\n    label_transform=transforms.Compose([transforms.Select('label')]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Datasets\n\nTorchEEG also supports custom datasets. You can place recorded EEG\nsignal files in a folder following a specific rule, such as:\n\n::\n\n   label01\n   |- sub01.edf\n   |- sub02.edf\n   label02\n   |- sub01.edf\n   |- sub02.edf\n\nThen, you can use ``FolderDataset`` to automatically access the\ncorresponding EEG signal samples.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torcheeg.datasets import FolderDataset\n\nlabel_map = {'label01': 0, 'label02': 1}\ndataset = FolderDataset(io_path='./folder',\n                        root_path='./root_folder',\n                        structure='subject_in_label',\n                        num_channel=14,\n                        online_transform=transforms.ToTensor(),\n                        label_transform=transforms.Compose([\n                            transforms.Select('label'),\n                            transforms.Lambda(lambda x: label_map[x])\n                        ]),\n                        num_worker=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, you can use a CSV file to specify more detailed\nmeta-information for reading:\n\n::\n\n   | subject_id | trial_id | label | file_path                 |\n   | ---------- | -------  | ----- | ------------------------- |\n   | sub1       | 0        | 0     | './data/label1/sub1.fif' |\n   | sub1       | 1        | 1     | './data/label2/sub1.fif' |\n   | sub1       | 2        | 2     | './data/label3/sub1.fif' |\n   | sub2       | 0        | 0     | './data/label1/sub2.fif' |\n   | sub2       | 1        | 1     | './data/label2/sub2.fif' |\n   | sub2       | 2        | 2     | './data/label3/sub2.fif' |\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torcheeg.datasets import CSVFolderDataset\n\ndataset = CSVFolderDataset(csv_path='./data.csv',\n                           online_transform=transforms.ToTensor(),\n                           label_transform=transforms.Select('label'),\n                           num_worker=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, TorchEEG uses mne to read recorded EEG signals, but you can\nalso specify your own file reading logic through ``read_fn``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mne\n\n\ndef default_read_fn(file_path, **kwargs):\n    # Load EEG file\n    raw = mne.io.read_raw(file_path)\n    # Convert raw to epochs\n    epochs = mne.make_fixed_length_epochs(raw, duration=1)\n    # Return EEG data\n    return epochs\n\n\ndataset = CSVFolderDataset(io_path='./csv_folder',\n                           csv_path='./data.csv',\n                           read_fn=default_read_fn,\n                           online_transform=transforms.ToTensor(),\n                           label_transform=transforms.Select('label'),\n                           num_worker=4)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}